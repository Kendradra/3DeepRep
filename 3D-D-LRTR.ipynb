{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim \n",
    "from torch.autograd import Variable \n",
    "import os \n",
    "from utils import * \n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import numpy as np \n",
    "import scipy.io as scio\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load npy files\n",
    "data_set = 'Pavia'\n",
    "case = 'MR_90' \n",
    "path2 = case + '/'\n",
    "img = np.load(data_set + '_''gtruth.npy')\n",
    "sampled_image = np.load(path2 + data_set + '_''sampled.npy')\n",
    "interplted_img = np.load(path2 + data_set + '_''init.npy')\n",
    "mask = np.load(path2 + data_set + '_''mask.npy')\n",
    "\n",
    "print('data set: ' + data_set)\n",
    "print('case: ' + case)\n",
    "\n",
    "#set data\n",
    "X_np = sampled_image \n",
    "X = torch.from_numpy(X_np).type(dtype).cuda()\n",
    "\n",
    "X_in_np = interplted_img \n",
    "X_in = torch.from_numpy(X_in_np).type(dtype).cuda()\n",
    "\n",
    "mask = torch.from_numpy(mask ).type(dtype)\n",
    "\n",
    "X_GTruth_np=img \n",
    "X_GTruth = torch.from_numpy(X_GTruth_np).type(dtype).cuda()\n",
    "\n",
    "psnr=psnr3d(X_GTruth_np,X_in_np)\n",
    "print(f'psnr of the initialized data: {psnr:.3f}')\n",
    "\n",
    "# set network parameters\n",
    "band_shrink = 1\n",
    "CNN_layer = 1\n",
    "CNN_size = 3\n",
    "\n",
    "# set some parameters\n",
    "label= '3D-D-LRTR_' + data_set + '_' + case\n",
    "path0 = \"\"\n",
    "\n",
    "n_1=X.shape[0]\n",
    "n_2=X.shape[1]\n",
    "n_3=X.shape[2]\n",
    "\n",
    "n_4=dict()\n",
    "n_4[\"tm1\"]=n_1*band_shrink\n",
    "n_4[\"tm2\"]=n_2*band_shrink\n",
    "n_4[\"tm3\"]=n_3*band_shrink\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define networks\n",
    "class coupled_block(nn.Module):\n",
    "    def __init__(self,n_4,n_0,CNN_layer,CNN_size):\n",
    "        super(coupled_block, self).__init__()\n",
    "        self.cnn_block=nn.Sequential(nn.Conv2d(n_4,n_4,CNN_size,padding=int((CNN_size-1)/2),groups=n_4,bias=False),\n",
    "                               nn.LeakyReLU())   \n",
    "        for l in range(CNN_layer-1):\n",
    "            self.cnn_block=nn.Sequential(self.cnn_block,\n",
    "                                         nn.Conv2d(n_4,n_4,CNN_size,padding=int((CNN_size-1)/2),groups=n_4,bias=False),\n",
    "                                         nn.LeakyReLU())      \n",
    "           \n",
    "        self.fc_block=nn.Sequential(nn.Linear(n_4,n_0, bias = False),\n",
    "                              nn.LeakyReLU())\n",
    "    def forward(self, x):\n",
    "        special_g=self.cnn_block(x.permute(2,0,1))\n",
    "        g_x=self.fc_block(special_g.permute(1,2,0))\n",
    "        return g_x\n",
    "class g1_Xhat(nn.Module): \n",
    "    def __init__(self,n_1,n_2,n_3,n_4,CNN_layer,CNN_size):\n",
    "        super(g1_Xhat, self).__init__()\n",
    "        self.X_hat = nn.Parameter(torch.Tensor(n_4,n_2,n_3))\n",
    "        self.g = coupled_block(n_4, n_1,CNN_layer,CNN_size)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.X_hat.size(0))\n",
    "        self.X_hat.data.uniform_(-stdv, stdv)\n",
    "                                    \n",
    "    def forward(self):\n",
    "        X_hat_pmted=self.X_hat.permute(1,2,0)\n",
    "        g_x=self.g(X_hat_pmted)\n",
    "        g_x_uprgt=g_x.permute(2,0,1)\n",
    "        return X_hat_pmted, g_x_uprgt\n",
    "class g2_Xhat(nn.Module): \n",
    "    def __init__(self,n_1,n_2,n_3,n_4,CNN_layer,CNN_size):\n",
    "        super(g2_Xhat, self).__init__()\n",
    "        self.X_hat = nn.Parameter(torch.Tensor(n_1,n_4,n_3))\n",
    "        self.g = coupled_block(n_4, n_2,CNN_layer,CNN_size)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.X_hat.size(0))\n",
    "        self.X_hat.data.uniform_(-stdv, stdv)\n",
    "                                    \n",
    "    def forward(self):\n",
    "        X_hat_pmted=self.X_hat.permute(0,2,1)\n",
    "        g_x=self.g(X_hat_pmted)\n",
    "        g_x_uprgt=g_x.permute(0,2,1)\n",
    "        return X_hat_pmted, g_x_uprgt\n",
    "class g3_Xhat(nn.Module): \n",
    "    def __init__(self,n_1,n_2,n_3,n_4,CNN_layer,CNN_size):\n",
    "        super(g3_Xhat, self).__init__()\n",
    "        self.X_hat = nn.Parameter(torch.Tensor(n_1,n_2,n_4))\n",
    "        self.g = coupled_block(n_4, n_3,CNN_layer,CNN_size)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.X_hat.size(0))\n",
    "        self.X_hat.data.uniform_(-stdv, stdv)\n",
    "                                    \n",
    "    def forward(self):\n",
    "        X_hat=self.X_hat\n",
    "        g_x_uprgt=self.g(self.X_hat)\n",
    "        return X_hat, g_x_uprgt\n",
    "class ThreeD_D_LRTR(nn.Module):\n",
    "    def __init__(self,n_1,n_2,n_3,n_4_tm1,n_4_tm2,n_4_tm3,CNN_layer,CNN_size):\n",
    "        super(ThreeD_D_LRTR, self).__init__()\n",
    "   \n",
    "        self.g_tm1 = g1_Xhat(n_1,n_2,n_3,n_4_tm1,CNN_layer,CNN_size)\n",
    "        self.g_tm2 = g2_Xhat(n_1,n_2,n_3,n_4_tm2,CNN_layer,CNN_size)\n",
    "        self.g_tm3 = g3_Xhat(n_1,n_2,n_3,n_4_tm3,CNN_layer,CNN_size)\n",
    "\n",
    "        self.g_CoNot_inte = coupled_block(n_3*3, n_3,CNN_layer,CNN_size)\n",
    "                                    \n",
    "    def forward(self):\n",
    "        X_hat_tm1_pmted, g_x_tm1_uprgt=self.g_tm1()\n",
    "        X_hat_tm2_pmted, g_x_tm2_uprgt=self.g_tm2()\n",
    "        X_hat_tm3_pmted, g_x_tm3_uprgt=self.g_tm3()\n",
    "\n",
    "        #concatenate g_x_tm1_uprgt, g_x_tm2_uprgt, g_x_tm3_uprgt\n",
    "        X_inte=torch.cat((g_x_tm1_uprgt, g_x_tm2_uprgt, g_x_tm3_uprgt),2)\n",
    "\n",
    "        #transform into the original domain\n",
    "        g_x_uprgt=self.g_CoNot_inte(X_inte)\n",
    "\n",
    "        return X_hat_tm1_pmted, \\\n",
    "            X_hat_tm2_pmted, \\\n",
    "                X_hat_tm3_pmted, \\\n",
    "                    g_x_uprgt,\\\n",
    "        g_x_tm1_uprgt, g_x_tm2_uprgt, g_x_tm3_uprgt\n",
    "F_norm = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_result(model_state, X_save, df, nucs,\\\n",
    "            losses, psnr_curve, iter):\n",
    "    now_time = datetime.datetime.now().strftime('%Y-%m-%d %H.%M')\n",
    "    path=label+\"_\"+now_time\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        now_time = datetime.datetime.now().strftime('%m-%d %H.%M.%S')\n",
    "        path=label+\"_\"+now_time\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    path_save=path0+path+\"/\"\n",
    "    df.to_csv(path_save + label+\"_\"+now_time+\".csv\",index=False)\n",
    "\n",
    "    start=int(iter/5)\n",
    "\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.savefig(path_save+\"Training Loss1\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(start+np.arange(np.array(losses).shape[0]-start),losses[start:])\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.savefig(path_save+\"Training Loss2\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(psnr_curve)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('PSNR')\n",
    "    plt.title('Training PSNR')\n",
    "    plt.savefig(path_save+\"Training PSNR1\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(start+np.arange(np.array(psnr_curve).shape[0]-start),psnr_curve[start:])\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('PSNR')\n",
    "    plt.title('Training PSNR')\n",
    "    plt.savefig(path_save+\"Training PSNR2\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(nucs)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.title('nuc')\n",
    "    plt.savefig(path_save+\"nuc_\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(start+np.arange(np.array(nucs).shape[0]-start),nucs[start:])\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.title('nuc')\n",
    "    plt.savefig(path_save+\"nuc_\")\n",
    "    plt.show()\n",
    "\n",
    "    torch.save({'model':model_state},path_save + label + '.pth')\n",
    "    np.save(path_save + label + '.npy',X_save)\n",
    "    \n",
    "def training(max_iter, lr, beta_, theta_, gamma, beta1):  \n",
    "    #training\n",
    "    \n",
    "    gamma_1 = gamma\n",
    "    gamma_2 = gamma\n",
    "    gamma_3 = gamma\n",
    "    \n",
    "\n",
    "    model, params = init_model()\n",
    "    optimizier = optim.Adam(params, lr=lr, betas=(beta1,0.999)) \n",
    "\n",
    "    psnr_curve = []\n",
    "    losses = []\n",
    "    fdltys = []\n",
    "    nucs = []\n",
    "    nuc1s = []\n",
    "    nuc2s = []\n",
    "    nuc3s = []\n",
    "    psnr_best = 0\n",
    "\n",
    "    start_time=time.time()\n",
    "    start_time_1 = time.time()\n",
    "    for iter in range(max_iter):\n",
    "        \n",
    "        X_LR1, X_LR2, X_LR3, X_Out, g1_X, g2_X, g3_X = model()\n",
    "        psnr = psnr3d(X_GTruth.cpu().detach().numpy(),X_Out.cpu().detach().numpy())\n",
    "        \n",
    "        if psnr > psnr_best:\n",
    "            psnr_best = psnr\n",
    "            X_save = X_Out.clone().cpu().detach().numpy()\n",
    "            model_to_save = copy.deepcopy(model).cpu()\n",
    "            model_state = model_to_save.state_dict() \n",
    "            \n",
    "        try:    \n",
    "            nuc1 =  torch.norm(X_LR1[:,:,int(iter%n_4[\"tm1\"])].cuda(),'nuc')\n",
    "        except:\n",
    "            nuc1 =  torch.norm(X_LR1[:,:,int((iter+1)%n_4[\"tm1\"])].cuda(),'nuc')\n",
    "        \n",
    "        try:    \n",
    "            nuc2 = torch.norm(X_LR2[:,:,int(iter%n_4[\"tm2\"])].cuda(),'nuc')\n",
    "        except:\n",
    "            nuc2 = torch.norm(X_LR2[:,:,int((iter+1)%n_4[\"tm2\"])].cuda(),'nuc')\n",
    "\n",
    "        try:    \n",
    "            nuc3 = torch.norm(X_LR3[:,:,int(iter%n_4[\"tm3\"])].cuda(),'nuc')\n",
    "        except:\n",
    "            nuc3 = torch.norm(X_LR3[:,:,int((iter+1)%n_4[\"tm3\"])].cuda(),'nuc')\n",
    "        #optimize the nuclear norm of only one frontal slice at one time\n",
    "\n",
    "        nuc = beta_*(nuc1 + nuc2 + theta_*nuc3)/(1+1+theta_)\n",
    "        \n",
    "        f1 = gamma_1*F_norm(g1_X*mask,X*mask)\n",
    "        f2 = gamma_2*F_norm(g2_X*mask,X*mask)\n",
    "        f3 = gamma_3*F_norm(g3_X*mask,X*mask)\n",
    "        \n",
    "        f = F_norm(X_Out*mask,X*mask)\n",
    "\n",
    "        loss = nuc + f + f1 + f2 + f3\n",
    "\n",
    "        optimizier.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizier.step()\n",
    "\n",
    "        psnr_curve.append(psnr.item())\n",
    "        losses.append(loss.item())\n",
    "        fdltys.append(f.item())\n",
    "        nucs.append(nuc.item())\n",
    "        nuc1s.append(nuc1.item())\n",
    "        nuc2s.append(nuc2.item())\n",
    "        nuc3s.append(nuc3.item())\n",
    "        \n",
    "\n",
    "        if iter % 40 == 0:\n",
    "            end_time=time.time()\n",
    "            iteration_time=end_time-start_time\n",
    "            start_time=time.time()\n",
    "            print('training: ' + label)\n",
    "            print('iter: {:}/{:}; time used: {:.3f}; PSNR: {:.3f}'.format(iter, max_iter, iteration_time, psnr))\n",
    "            print()\n",
    "            \n",
    "            \n",
    "    end_time_1 = time.time()\n",
    "    training_time = end_time_1-start_time_1\n",
    "    training_time = round(training_time,2)\n",
    "    \n",
    "    X_LR1, X_LR2, X_LR3, X_Out, g1_X, g2_X, g3_X = model()\n",
    "    psnr = psnr3d(X_GTruth.cpu().detach().numpy(),X_Out.cpu().detach().numpy())\n",
    "    psnr_max = max(psnr_curve)\n",
    "\n",
    "    df = pd.DataFrame(list(zip([lr],[beta1], [beta_], [theta_],[ gamma], \\\n",
    "    [band_shrink],[CNN_layer],[CNN_size],\\\n",
    "    [iter], [training_time],[psnr],[psnr_max])),\\\n",
    "    columns =['learning rate','beta1','beta','theta','gamma',\\\n",
    "    'band_shrink','CNN_layer',\\\n",
    "    'CNN_size', 'iter','time_used', 'PSNR_stop','PSNR_max'])\n",
    "\n",
    "    save_training_result(model_state, X_save, df, nucs,\\\n",
    "            losses, psnr_curve, iter)\n",
    "            \n",
    "    return model\n",
    "def init_model():\n",
    "\n",
    "    n_4_tm1=n_4[\"tm1\"]\n",
    "    n_4_tm2=n_4[\"tm2\"]\n",
    "    n_4_tm3=n_4[\"tm3\"]\n",
    "\n",
    "    model = ThreeD_D_LRTR(n_1,n_2,n_3,n_4_tm1,n_4_tm2,n_4_tm3,CNN_layer,CNN_size).type(dtype)\n",
    "\n",
    "    #initialize X_hat\n",
    "    model.g_tm1.X_hat.data = X_in.cpu().detach().cuda()\n",
    "    model.g_tm2.X_hat.data = X_in.cpu().detach().cuda()\n",
    "    model.g_tm3.X_hat.data = X_in.cpu().detach().cuda()\n",
    "\n",
    "    params = []\n",
    "    params += [x for x in model.parameters()] \n",
    "\n",
    "    s = sum([np.prod(list(p.size())) for p in params]); \n",
    "    print('Number of params: %d' % s)\n",
    "\n",
    "    return model, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "max_iter = 3000 #120000\n",
    "lr = math.pow(10,-2)\n",
    "beta_ = math.pow(10, -6)\n",
    "theta_ = math.pow(10,-1)\n",
    "gamma = math.pow(10, -7)\n",
    "beta1 = 0.99\n",
    "\n",
    "model = training(max_iter, \\\n",
    "    lr, beta_, theta_, gamma,beta1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
